# T&C Clarity - Legal Terms AI Summarizer

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)](https://fastapi.tiangolo.com/)
[![LangChain](https://img.shields.io/badge/LangChain-0.1+-orange.svg)](https://www.langchain.com/)

An intelligent application that analyzes Terms & Conditions documents to extract key information, identify hidden clauses, and provide plain-language summaries. Never blindly accept T&Cs again!

## ðŸŽ¯ Project Overview

This LLMOps project demonstrates production-ready implementation of:
- **OpenAPI** - RESTful API design and documentation
- **Hugging Face** - LLM models for summarization and embeddings
- **Vector Database** - Semantic search using ChromaDB
- **LangChain** - LLM workflow orchestration
- **MLOps Best Practices** - Monitoring, versioning, and evaluation

## âœ¨ Features

### Core Capabilities
- ðŸ“„ **Multi-format Support** - Process PDF, text files, and web URLs
- ðŸŽ¯ **Smart Summarization** - Generate concise executive summaries
- ðŸš© **Red Flag Detection** - Identify concerning clauses automatically
- ðŸ“Š **Risk Scoring** - Calculate overall risk score (0-100)
- ðŸ’¬ **Interactive Q&A** - RAG-powered question answering
- ðŸ” **Semantic Search** - Find similar clauses across documents
- ðŸ“‘ **Section Classification** - Automatically categorize T&C sections

### LLMOps Features
- ðŸ”„ **Experiment Tracking** - MLflow integration
- ðŸ“ˆ **Monitoring** - Prometheus + Grafana dashboards
- ðŸ§ª **Automated Testing** - Evaluation metrics (ROUGE, accuracy)
- ðŸš€ **CI/CD Pipeline** - GitHub Actions automation
- ðŸ“ **Prompt Versioning** - Git-based prompt management
- âš¡ **Performance Optimization** - Redis caching

## ðŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend/CLI  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   FastAPI       â”‚
    â”‚   REST API      â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚    LangChain Orchestration  â”‚
    â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚             â”‚
   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ HF     â”‚   â”‚  ChromaDB     â”‚
   â”‚ Models â”‚   â”‚  Vector Store â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚             â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”
    â”‚   PostgreSQL DB      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸš€ Quick Start

### Prerequisites
- Python 3.9+
- Docker & Docker Compose
- Git
- Hugging Face API key (free tier available)

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/NihalKA/legal-terms-ai-summarizer.git
cd legal-terms-ai-summarizer
```

2. **Set up environment**
```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

3. **Configure environment variables**
```bash
cp .env.example .env
# Edit .env with your settings:
# - HUGGINGFACE_API_KEY
# - DATABASE_URL
# - REDIS_URL
```

4. **Start services with Docker**
```bash
docker-compose up -d
```

5. **Run migrations**
```bash
alembic upgrade head
```

6. **Start the API server**
```bash
uvicorn app.main:app --reload
```

Visit `http://localhost:8000/docs` for interactive API documentation.

## ðŸ“– Usage

### API Endpoints

#### Upload and Analyze Document
```bash
curl -X POST "http://localhost:8000/api/v1/analyze" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@terms.pdf"
```

#### Query Document with RAG
```bash
curl -X POST "http://localhost:8000/api/v1/query" \
  -H "Content-Type: application/json" \
  -d '{"document_id": "123", "question": "What data do they collect?"}'
```

#### Get Risk Analysis
```bash
curl -X GET "http://localhost:8000/api/v1/documents/123/risks"
```

### Python SDK Example
```python
from tc_clarity import TCClarityClient

client = TCClarityClient(api_key="your-api-key")

# Analyze document
result = client.analyze_document("path/to/terms.pdf")

print(f"Risk Score: {result.risk_score}/100")
print(f"Summary: {result.summary}")
print(f"Red Flags: {result.red_flags}")

# Ask questions
answer = client.query("What is the refund policy?", result.document_id)
print(answer.text)
```

## ðŸ› ï¸ Technology Stack

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **API Framework** | FastAPI | REST API with OpenAPI docs |
| **LLM Orchestration** | LangChain | Chain workflows, RAG |
| **Models** | Hugging Face | BART, Sentence Transformers |
| **Vector DB** | ChromaDB | Semantic search |
| **Database** | PostgreSQL | Document metadata |
| **Cache** | Redis | Semantic caching |
| **Monitoring** | Prometheus + Grafana | Metrics & dashboards |
| **Experiment Tracking** | MLflow | Model versioning |
| **Tracing** | LangSmith | LLM call debugging |

## ðŸ“Š Project Structure

```
tc-clarity/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ v1/
â”‚   â”‚   â”‚   â”œâ”€â”€ endpoints/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ analyze.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ query.py
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ documents.py
â”‚   â”‚   â”‚   â””â”€â”€ router.py
â”‚   â”‚   â””â”€â”€ dependencies.py
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â”œâ”€â”€ security.py
â”‚   â”‚   â””â”€â”€ logging.py
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ document_processor.py
â”‚   â”‚   â”œâ”€â”€ summarizer.py
â”‚   â”‚   â”œâ”€â”€ risk_analyzer.py
â”‚   â”‚   â””â”€â”€ rag_engine.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ database.py
â”‚   â”‚   â””â”€â”€ schemas.py
â”‚   â”œâ”€â”€ chains/
â”‚   â”‚   â”œâ”€â”€ summarization_chain.py
â”‚   â”‚   â”œâ”€â”€ qa_chain.py
â”‚   â”‚   â””â”€â”€ risk_detection_chain.py
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ integration/
â”‚   â””â”€â”€ evaluation/
â”‚       â”œâ”€â”€ test_datasets/
â”‚       â””â”€â”€ metrics.py
â”œâ”€â”€ monitoring/
â”‚   â”œâ”€â”€ prometheus/
â”‚   â”‚   â””â”€â”€ prometheus.yml
â”‚   â””â”€â”€ grafana/
â”‚       â””â”€â”€ dashboards/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup_db.py
â”‚   â”œâ”€â”€ seed_data.py
â”‚   â””â”€â”€ evaluate_model.py
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ API.md
â”‚   â”œâ”€â”€ ARCHITECTURE.md
â”‚   â”œâ”€â”€ DEPLOYMENT.md
â”‚   â””â”€â”€ CONTRIBUTING.md
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ ci.yml
â”‚       â”œâ”€â”€ deploy.yml
â”‚       â””â”€â”€ evaluate.yml
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

## ðŸ§ª Testing & Evaluation

### Run Tests
```bash
# Unit tests
pytest tests/unit -v

# Integration tests
pytest tests/integration -v

# Evaluation metrics
python scripts/evaluate_model.py
```

### Evaluation Metrics
- **Summarization**: ROUGE-L > 0.4
- **Red Flag Detection**: Accuracy > 85%
- **RAG Answer Relevance**: > 80%
- **API Latency**: p95 < 2 seconds

## ðŸ“ˆ Monitoring

Access monitoring dashboards:
- **Grafana**: http://localhost:3000
- **Prometheus**: http://localhost:9090
- **MLflow**: http://localhost:5000

Key metrics tracked:
- API request latency
- Model inference time
- Token usage
- Cache hit rate
- Error rates
- Red flag detection accuracy

## ðŸš¢ Deployment

### Docker Deployment
```bash
# Build image
docker build -t tc-clarity:latest .

# Run container
docker run -p 8000:8000 --env-file .env tc-clarity:latest
```

### Cloud Deployment
See [DEPLOYMENT.md](docs/DEPLOYMENT.md) for:
- AWS deployment guide
- GCP deployment guide
- Azure deployment guide
- Kubernetes manifests

## ðŸ“š Documentation

- [API Documentation](docs/API.md) - Complete API reference
- [Architecture Guide](docs/ARCHITECTURE.md) - System design details
- [Deployment Guide](docs/DEPLOYMENT.md) - Production deployment
- [Contributing Guide](docs/CONTRIBUTING.md) - Development guidelines

## ðŸ—ºï¸ Roadmap

### Phase 1: Core Features (Weeks 1-6) âœ…
- [x] Document processing pipeline
- [x] Basic summarization
- [x] Vector database integration
- [x] RAG implementation

### Phase 2: Analysis Engine (Weeks 7-9) âœ…
- [x] Red flag detection
- [x] Risk scoring
- [x] Section classification

### Phase 3: LLMOps (Weeks 10-12) ðŸ”„
- [x] Monitoring setup
- [x] Evaluation framework
- [ ] CI/CD pipeline
- [ ] Automated testing

### Phase 4: Advanced Features (Future)
- [ ] Multi-language support
- [ ] Comparison mode (multiple T&Cs)
- [ ] Browser extension
- [ ] Mobile app

## ðŸ¤ Contributing

Contributions are welcome! Please see [CONTRIBUTING.md](docs/CONTRIBUTING.md) for guidelines.

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ðŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ðŸ™ Acknowledgments

- Hugging Face for providing excellent pre-trained models
- LangChain community for orchestration framework
- FastAPI for the amazing web framework
- ChromaDB for vector database capabilities

## ðŸ“§ Contact

**Project Maintainer**: Nihal KA
- GitHub: [@NihalKA](https://github.com/NihalKA)
- Project Link: [https://github.com/NihalKA/legal-terms-ai-summarizer](https://github.com/NihalKA/legal-terms-ai-summarizer)

## ðŸ”— Related Projects

- [LangChain](https://github.com/langchain-ai/langchain)
- [ChromaDB](https://github.com/chroma-core/chroma)
- [FastAPI](https://github.com/tiangolo/fastapi)

---

â­ **Star this repo** if you find it helpful!

ðŸ“ **Report issues** via GitHub Issues

ðŸ’¡ **Share feedback** to help improve the project