# T&C Clarity - Legal Terms AI Summarizer

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)](https://fastapi.tiangolo.com/)
[![LangChain](https://img.shields.io/badge/LangChain-0.1+-orange.svg)](https://www.langchain.com/)

An intelligent application that analyzes Terms & Conditions documents to extract key information, identify hidden clauses, and provide plain-language summaries. Never blindly accept T&Cs again!

## ðŸŽ¯ Project Overview

This LLMOps project demonstrates production-ready implementation of:
- **OpenAPI** - RESTful API design and documentation
- **Hugging Face** - LLM models for summarization and embeddings
- **Vector Database** - Semantic search using ChromaDB
- **LangChain** - LLM workflow orchestration
- **MLOps Best Practices** - Monitoring, versioning, and evaluation

## âœ¨ Features

### Core Capabilities
- ðŸ“„ **Multi-format Support** - Process PDF, text files, and web URLs
- ðŸŽ¯ **Smart Summarization** - Generate concise executive summaries
- ðŸš© **Red Flag Detection** - Identify concerning clauses automatically
- ðŸ“Š **Risk Scoring** - Calculate overall risk score (0-100)
- ðŸ’¬ **Interactive Q&A** - RAG-powered question answering
- ðŸ” **Semantic Search** - Find similar clauses across documents
- ðŸ“‘ **Section Classification** - Automatically categorize T&C sections

### LLMOps Features
- ðŸ”„ **Experiment Tracking** - MLflow integration
- ðŸ“ˆ **Monitoring** - Prometheus + Grafana dashboards
- ðŸ§ª **Automated Testing** - Evaluation metrics (ROUGE, accuracy)
- ðŸš€ **CI/CD Pipeline** - GitHub Actions automation
- ðŸ“ **Prompt Versioning** - Git-based prompt management
- âš¡ **Performance Optimization** - Redis caching

## ðŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend/CLI  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   FastAPI       â”‚
    â”‚   REST API      â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚    LangChain Orchestration  â”‚
    â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚             â”‚
   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ HF     â”‚   â”‚  ChromaDB     â”‚
   â”‚ Models â”‚   â”‚  Vector Store â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚             â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”
    â”‚   PostgreSQL DB      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸš€ Quick Start

### Prerequisites
- Python 3.10+
- Docker & Docker Compose
- Git
- Hugging Face API key (free tier available)

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/NihalKA/legal-terms-ai-summarizer.git
cd legal-terms-ai-summarizer
```

2. **Set up environment**
```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

3. **Configure environment variables**
```bash
cp .env.example .env
# Edit .env with your settings:
# - HUGGINGFACE_API_KEY
# - LANGCHAIN_API_KEY
# - DATABASE_URL
# - REDIS_URL
```

4. **Start services with Docker**
```bash
docker-compose up -d
```

5. **Initialize database**
```bash
python scripts/init_db.py
```

6. **Start the API server**
```bash
uvicorn src.api.main:app --reload
```

Visit `http://localhost:8000/docs` for interactive API documentation.

## ðŸ“– Usage

### API Endpoints

#### Upload and Analyze Document
```bash
curl -X POST "http://localhost:8000/api/v1/analyze" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@terms.pdf"
```

#### Query Document with RAG
```bash
curl -X POST "http://localhost:8000/api/v1/query" \
  -H "Content-Type: application/json" \
  -d '{"document_id": "123", "question": "What data do they collect?"}'
```

#### Get Risk Analysis
```bash
curl -X GET "http://localhost:8000/api/v1/documents/123/risks"
```

### Python SDK Example
```python
from tc_clarity import TCClarityClient

client = TCClarityClient(api_key="your-api-key")

# Analyze document
result = client.analyze_document("path/to/terms.pdf")

print(f"Risk Score: {result.risk_score}/100")
print(f"Summary: {result.summary}")
print(f"Red Flags: {result.red_flags}")

# Ask questions
answer = client.query("What is the refund policy?", result.document_id)
print(answer.text)
```

## ðŸ› ï¸ Technology Stack

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **API Framework** | FastAPI | REST API with OpenAPI docs |
| **LLM Orchestration** | LangChain | Chain workflows, RAG |
| **Models** | Hugging Face | BART, Sentence Transformers |
| **Vector DB** | ChromaDB | Semantic search |
| **Database** | PostgreSQL | Document metadata |
| **Cache** | Redis | Semantic caching |
| **Monitoring** | Prometheus + Grafana | Metrics & dashboards |
| **Experiment Tracking** | MLflow | Model versioning |
| **Tracing** | LangSmith | LLM call debugging |

## ðŸ“Š Project Structure

```
tc-clarity/
â”œâ”€â”€ data/                    # All data storage
â”‚   â”œâ”€â”€ raw/                # Raw uploaded documents
â”‚   â”œâ”€â”€ processed/          # Processed/cleaned data
â”‚   â”œâ”€â”€ vector_store/       # ChromaDB vector store
â”‚   â””â”€â”€ knowledge_base/     # Curated knowledge base
â”œâ”€â”€ src/                    # Source code
â”‚   â”œâ”€â”€ api/               # FastAPI application
â”‚   â”‚   â”œâ”€â”€ main.py       # API entry point
â”‚   â”‚   â”œâ”€â”€ routes/       # API endpoints
â”‚   â”‚   â””â”€â”€ dependencies.py
â”‚   â”œâ”€â”€ rag/              # RAG implementation
â”‚   â”‚   â”œâ”€â”€ retriever.py
â”‚   â”‚   â”œâ”€â”€ generator.py
â”‚   â”‚   â””â”€â”€ embeddings.py
â”‚   â”œâ”€â”€ preprocessing/    # Document processing
â”‚   â”‚   â”œâ”€â”€ pdf_parser.py
â”‚   â”‚   â”œâ”€â”€ text_cleaner.py
â”‚   â”‚   â””â”€â”€ chunker.py
â”‚   â”œâ”€â”€ evaluation/       # Metrics & evaluation
â”‚   â”‚   â”œâ”€â”€ metrics.py
â”‚   â”‚   â””â”€â”€ evaluators.py
â”‚   â”œâ”€â”€ monitoring/       # Observability
â”‚   â”‚   â”œâ”€â”€ metrics.py
â”‚   â”‚   â””â”€â”€ logging.py
â”‚   â”œâ”€â”€ database/         # Database models
â”‚   â”‚   â”œâ”€â”€ models.py
â”‚   â”‚   â””â”€â”€ connection.py
â”‚   â””â”€â”€ utils/            # Utility functions
â”œâ”€â”€ models/               # Downloaded model files
â”‚   â”œâ”€â”€ embeddings/
â”‚   â””â”€â”€ summarization/
â”œâ”€â”€ prompts/              # Versioned prompts
â”‚   â”œâ”€â”€ summarization.py
â”‚   â”œâ”€â”€ risk_detection.py
â”‚   â””â”€â”€ qa.py
â”œâ”€â”€ tests/                # Test suites
â”‚   â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ integration/
â”‚   â””â”€â”€ evaluation/
â”œâ”€â”€ notebooks/            # Jupyter notebooks
â”œâ”€â”€ infra/                # Infrastructure as code
â”‚   â”œâ”€â”€ terraform/
â”‚   â””â”€â”€ kubernetes/
â”œâ”€â”€ configs/              # Configuration files
â”‚   â”œâ”€â”€ prometheus.yml
â”‚   â””â”€â”€ grafana/
â”œâ”€â”€ scripts/              # Utility scripts
â”‚   â”œâ”€â”€ init_db.py
â”‚   â”œâ”€â”€ seed_data.py
â”‚   â””â”€â”€ evaluate_model.py
â”œâ”€â”€ docs/                 # Documentation
â”‚   â”œâ”€â”€ API.md
â”‚   â”œâ”€â”€ ARCHITECTURE.md
â”‚   â”œâ”€â”€ DEPLOYMENT.md
â”‚   â””â”€â”€ CONTRIBUTING.md
â”œâ”€â”€ .github/              # CI/CD workflows
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ ci.yml
â”‚       â”œâ”€â”€ deploy.yml
â”‚       â””â”€â”€ evaluate.yml
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

**Structure Explanation:**

- `data/` - All data storage (documents, embeddings, knowledge base)
- `src/` - All source code organized by function
- `models/` - Downloaded/trained model files
- `prompts/` - Version-controlled prompt templates
- `tests/` - Test suites (unit, integration, evaluation)
- `notebooks/` - Jupyter notebooks for experimentation
- `infra/` - Infrastructure as code (Terraform, K8s configs)
- `configs/` - Configuration files (Prometheus, Grafana)

## ðŸ§ª Testing & Evaluation

### Run Tests
```bash
# Unit tests
pytest tests/unit -v

# Integration tests
pytest tests/integration -v

# Evaluation metrics
python scripts/evaluate_model.py
```

### Evaluation Metrics
- **Summarization**: ROUGE-L > 0.4
- **Red Flag Detection**: Accuracy > 85%
- **RAG Answer Relevance**: > 80%
- **API Latency**: p95 < 2 seconds

## ðŸ“ˆ Monitoring

Access monitoring dashboards:
- **API Docs**: http://localhost:8000/docs
- **MLflow**: http://localhost:5000
- **Grafana**: http://localhost:3000 (admin/admin)
- **Prometheus**: http://localhost:9090

Key metrics tracked:
- API request latency
- Model inference time
- Token usage
- Cache hit rate
- Error rates
- Red flag detection accuracy

## ðŸš¢ Deployment

### Docker Deployment
```bash
# Build image
docker build -t tc-clarity:latest .

# Run container
docker run -p 8000:8000 --env-file .env tc-clarity:latest
```

### Cloud Deployment
See [DEPLOYMENT.md](docs/DEPLOYMENT.md) for:
- AWS deployment guide
- GCP deployment guide
- Azure deployment guide
- Kubernetes manifests

## ðŸ“š Documentation

- **[Phase 0: Setup Guide](https://ernihalka.atlassian.net/wiki/spaces/LLMOps/pages/426051)** - Complete setup instructions
- [API Documentation](docs/API.md) - Complete API reference
- [Architecture Guide](docs/ARCHITECTURE.md) - System design details
- [Deployment Guide](docs/DEPLOYMENT.md) - Production deployment
- [Contributing Guide](docs/CONTRIBUTING.md) - Development guidelines

## ðŸ—ºï¸ Roadmap

### Phase 0: Setup (Week 1) âœ…
- [x] Project structure
- [x] Development environment
- [x] Docker services

### Phase 1: Core Infrastructure (Week 2) ðŸ”„
- [ ] Database models
- [ ] API skeleton
- [ ] Configuration management

### Phase 2: Document Processing (Week 3)
- [ ] PDF extraction
- [ ] Text cleaning
- [ ] Chunking logic

### Phase 3: Embedding & Vector Store (Week 4)
- [ ] Hugging Face integration
- [ ] Generate embeddings
- [ ] Chroma setup

### Phase 4: RAG Pipeline (Week 5)
- [ ] LangChain chains
- [ ] Retrieval logic
- [ ] Response generation

### Phase 5: Analysis Engine (Week 6)
- [ ] Summarization
- [ ] Red flag detection
- [ ] Risk scoring

### Phase 6: API Endpoints (Week 7)
- [ ] Upload endpoint
- [ ] Analysis endpoint
- [ ] Query endpoint

### Phase 7: Caching & Optimization (Week 8)
- [ ] Redis integration
- [ ] Semantic caching

### Phase 8: Monitoring Setup (Week 9)
- [ ] Prometheus metrics
- [ ] Grafana dashboards
- [ ] LangSmith tracing

### Phase 9: Evaluation Framework (Week 10)
- [ ] Test dataset creation
- [ ] Metrics implementation
- [ ] MLflow integration

### Phase 10: CI/CD Pipeline (Week 11)
- [ ] GitHub Actions
- [ ] Automated testing
- [ ] Deployment automation

### Phase 11: Frontend (Week 12) - Optional
- [ ] Streamlit UI
- [ ] Upload interface
- [ ] Results display

### Phase 12: Documentation & Polish (Week 13)
- [ ] Complete documentation
- [ ] Demo video
- [ ] Project presentation

## ðŸ¤ Contributing

Contributions are welcome! Please see [CONTRIBUTING.md](docs/CONTRIBUTING.md) for guidelines.

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ðŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ðŸ™ Acknowledgments

- Hugging Face for providing excellent pre-trained models
- LangChain community for orchestration framework
- FastAPI for the amazing web framework
- ChromaDB for vector database capabilities

## ðŸ“§ Contact

**Project Maintainer**: Nihal KA
- GitHub: [@NihalKA](https://github.com/NihalKA)
- Confluence: [LLMOps Space](https://ernihalka.atlassian.net/wiki/spaces/LLMOps)
- Jira: [KAN Project](https://ernihalka.atlassian.net/jira/software/projects/KAN)

## ðŸ”— Related Projects

- [LangChain](https://github.com/langchain-ai/langchain)
- [ChromaDB](https://github.com/chroma-core/chroma)
- [FastAPI](https://github.com/tiangolo/fastapi)

---

â­ **Star this repo** if you find it helpful!

ðŸ“ **Report issues** via GitHub Issues

ðŸ’¡ **Share feedback** to help improve the project

**Complete Setup Guide**: See [Phase 0 in Confluence](https://ernihalka.atlassian.net/wiki/spaces/LLMOps/pages/426051) for detailed setup instructions